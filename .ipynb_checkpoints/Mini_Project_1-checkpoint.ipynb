{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eslam\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2013train.txt\", sep='\\t', header=None)\n",
    "file_2 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2015train.txt\", sep='\\t', header=None)\n",
    "file_3 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2016train.txt\", sep='\\t', header=None)\n",
    "all_files = pd.concat([file_1, file_2, file_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16041\n",
       "1    16041\n",
       "2    16041\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_train,sms_test,train_labels,test_labels = train_test_split(all_files[2],all_files[1], test_size=0.25,random_state=1)\n",
    "\n",
    "all_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessCorpus(corpus):\n",
    "    lower_case_docs =[]\n",
    "    for doc in corpus:\n",
    "        lower_case_docs.append(doc.casefold())\n",
    "\n",
    "    stops = set(stopwords.words(\"D:/NU/Data Mining/Mini Project/stopwords.txt\"))\n",
    "    filtered_docs =[]\n",
    "    for doc in lower_case_docs:\n",
    "        k = \"\"\n",
    "        for word in  re.split(\"\\W+\",doc):\n",
    "            if word not in stops: \n",
    "                k = k + word +\" \"\n",
    "        k = k.strip()\n",
    "        filtered_docs.append(k)\n",
    "    \n",
    "    stemmed_docs =[]\n",
    "    for doc in  filtered_docs:\n",
    "        m = \"\"\n",
    "        for word in doc.split():  \n",
    "            m = m + PorterStemmer().stem(word) +\" \"\n",
    "        m = m.strip()\n",
    "        stemmed_docs.append(m)\n",
    "    return  stemmed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre_Files = preprocessCorpus(sms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
