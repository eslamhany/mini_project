{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from scipy import sparse\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2013train.txt\", sep='\\t', header=None)\n",
    "file_2 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2015train.txt\", sep='\\t', header=None)\n",
    "file_3 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2016train.txt\", sep='\\t', header=None)\n",
    "t_test = pd.read_csv(\"D:/NU/Data Mining/Mini Project/new_english_test.csv\", sep=',')\n",
    "Stop_Words = pd.read_csv(\"D:/NU/Data Mining/Mini Project/stopwords.txt\",header=None)\n",
    "all_files = pd.concat([file_1, file_2, file_3], ignore_index=True)\n",
    "All_Data = all_files.drop(all_files.columns[[0]], axis=1)\n",
    "All_Data_2 = All_Data.drop(all_files.columns[[1]], axis=1)\n",
    "Stop_Words = Stop_Words.iloc[3:]\n",
    "Stop_Words.columns = ['Words']\n",
    "StopWords = Stop_Words['Words'].tolist()\n",
    "All_Data_2.columns = ['Tweet']\n",
    "test = t_test.drop(t_test.columns[[0]], axis=1)\n",
    "test.columns = ['Tweet']\n",
    "for a in StopWords:\n",
    "    if isinstance(a, str):\n",
    "        text = a\n",
    "        decoded = False\n",
    "    else:\n",
    "        text = a.decode(encoding)\n",
    "        decoded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(line):\n",
    "    line = line.lower()\n",
    "    curr = \"\"\n",
    "    for word in line.split():\n",
    "        word = re.sub(r'\\\\[u][0-9]{1,}', '', word)\n",
    "        word = re.sub(r'@[a-zA-Z0-9_.-]{1,}','',word)\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", ' ', word)\n",
    "        if word not in StopWords:\n",
    "            curr = curr + word +\" \"\n",
    "    curr = curr.strip()\n",
    "    curr = PorterStemmer().stem(curr)\n",
    "    curr = curr.strip()\n",
    "    return curr\n",
    "\n",
    "def CountEmo_pos(line):\n",
    "    postive = line.count(':)')\n",
    "    postive = postive + line.count(':D')\n",
    "    postive = postive + line.count('=)')\n",
    "    postive = postive + line.count(':*')\n",
    "    postive = postive + line.count(':-)')\n",
    "    postive = postive + line.count('<3')\n",
    "    return postive\n",
    "\n",
    "def CountEmo_neg(line):\n",
    "    negative = line.count(':(')\n",
    "    negative = negative + line.count(':_(')\n",
    "    negative = negative + line.count(':''(')\n",
    "    negative = negative + line.count('=(')\n",
    "    negative = negative + line.count(':-(')\n",
    "    negative = negative + line.count(':S')\n",
    "    return negative\n",
    "\n",
    "def CountHashtags (line):\n",
    "    count = line.count('#')\n",
    "    return count\n",
    "\n",
    "def CountURLS (line):\n",
    "    count = line.count('http')\n",
    "    return count\n",
    "\n",
    "def CountMentions (line):\n",
    "    count = line.count('#')\n",
    "    return count\n",
    "                                         \n",
    "def CountExclamationMark(line):\n",
    "    count = line.count('!')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_2['Positive_Emoticons'] = All_Data_2['Tweet'].apply(CountEmo_pos)\n",
    "All_Data_2['Negative_Emoticons'] = All_Data_2['Tweet'].apply(CountEmo_neg)\n",
    "All_Data_2['HashTags'] = All_Data_2['Tweet'].apply(CountHashtags)\n",
    "All_Data_2['Mentions'] = All_Data_2['Tweet'].apply(CountMentions)\n",
    "All_Data_2['URLS'] = All_Data_2['Tweet'].apply(CountURLS)\n",
    "All_Data_2['EM'] = All_Data_2['Tweet'].apply(CountExclamationMark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_2['Tweet'] = All_Data_2['Tweet'].apply(PreProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_labels,test_labels = train_test_split(All_Data_2,All_Data[1], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pos_emotions = pd.DataFrame(np.array(train_data.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "t_neg_emotions = pd.DataFrame(np.array(train_data.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "t_thashtags = pd.DataFrame(np.array(train_data.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "t_mentions = pd.DataFrame(np.array(train_data.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "t_urls = pd.DataFrame(np.array(train_data.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "t_em = pd.DataFrame(np.array(train_data.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))\n",
    "\n",
    "s_pos_emotions = pd.DataFrame(np.array(test_data.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "s_neg_emotions = pd.DataFrame(np.array(test_data.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "s_thashtags = pd.DataFrame(np.array(test_data.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "s_mentions = pd.DataFrame(np.array(test_data.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "s_urls = pd.DataFrame(np.array(test_data.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "s_em = pd.DataFrame(np.array(test_data.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    vectorizer = CountVectorizer(min_df=2 ).fit(train_data['Tweet'])\n",
    "    train_data_vectorized = vectorizer.transform(train_data['Tweet'])\n",
    "    test_data_vectorized = vectorizer.transform(test_data['Tweet'])\n",
    "\n",
    "    train_d_vec = pd.DataFrame(train_data_vectorized.todense())\n",
    "    test_d_vec = pd.DataFrame(test_data_vectorized.todense())\n",
    "\n",
    "    train_d = pd.concat([train_d_vec, t_pos_emotions], axis=1)\n",
    "    train_dd = pd.concat([train_d, t_thashtags], axis=1)\n",
    "    train_ddd = pd.concat([train_dd, t_mentions], axis=1)\n",
    "    train_dddd = pd.concat([train_ddd, t_urls], axis=1)\n",
    "    train_ddddd = pd.concat([train_dddd, t_em], axis=1)\n",
    "    train_dddddd = pd.concat([train_ddddd, t_neg_emotions], axis=1)\n",
    "\n",
    "    test_d = pd.concat([test_d_vec, s_pos_emotions], axis=1)\n",
    "    test_dd = pd.concat([test_d, s_thashtags], axis=1)\n",
    "    test_ddd = pd.concat([test_dd, s_mentions], axis=1)\n",
    "    test_dddd = pd.concat([test_ddd, s_urls], axis=1)\n",
    "    test_ddddd = pd.concat([test_dddd, s_em], axis=1)\n",
    "    test_dddddd = pd.concat([test_ddddd, s_neg_emotions], axis=1)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression()],\n",
    "                     'classifier__penalty': ['l1', 'l2'],\n",
    "                     'classifier__C': np.logspace(0, 4, 10)},\n",
    "                    {'classifier': [RandomForestClassifier()],\n",
    "                     'classifier__n_estimators': [10, 100, 1000],\n",
    "                     'classifier__max_features': [1, 2, 3]}]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "    best_model = clf.fit(train_dddddd, train_labels)\n",
    "\n",
    "    print (best_model.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess_for_w2v(line):\n",
    "    line = line.lower()\n",
    "    curr = \"\"\n",
    "    for word in line.split():\n",
    "        word = re.sub(r'\\\\[u][0-9]{1,}', '', word)\n",
    "        word = re.sub(r'@[a-zA-Z0-9_.-]{1,}','',word)\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", ' ', word)\n",
    "        if word not in StopWords:\n",
    "            curr = curr + word +\" \"\n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Positive_Emoticons'] = test['Tweet'].apply(CountEmo_pos)\n",
    "test['Negative_Emoticons'] = test['Tweet'].apply(CountEmo_neg)\n",
    "test['HashTags'] = test['Tweet'].apply(CountHashtags)\n",
    "test['Mentions'] = test['Tweet'].apply(CountMentions)\n",
    "test['URLS'] = test['Tweet'].apply(CountURLS)\n",
    "test['EM'] = test['Tweet'].apply(CountExclamationMark)\n",
    "\n",
    "test['Tweet'] = test['Tweet'].apply(PreProcess)\n",
    "\n",
    "w2v_pre_train = pd.DataFrame(All_Data[2].apply(PreProcess_for_w2v))\n",
    "w2v_pre_test = pd.DataFrame(test['Tweet'].apply(PreProcess_for_w2v))\n",
    "\n",
    "w2v_pre_train.columns = ['Tweet']\n",
    "w2v_pre_test.columns = ['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('D:/NU/Data Mining/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All_Data_2['Tweet'] = All_Data_2['Tweet'].apply(PreProcess_for_w2v)\n",
    "test['Tweet'] = test['Tweet'].apply(PreProcess_for_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(df):\n",
    "    dd_ff = []\n",
    "    for i in df:\n",
    "        ww = \"\"\n",
    "        for x in i.split():\n",
    "            if not (x.isdigit() or x[0] == '-' and x[1:].isdigit()):\n",
    "                ww = ww + x +\" \"\n",
    "        dd_ff.append(ww)\n",
    "    n_data = pd.DataFrame(dd_ff)\n",
    "    return dd_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDF(corpus, xx, yy):\n",
    "    vectorizer = CountVectorizer()\n",
    "    cvect = vectorizer.fit_transform(corpus)\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    TF = cvect.toarray()\n",
    "    \n",
    "    document_frequency = []\n",
    "    for i in range(yy):\n",
    "        r = 0\n",
    "        for j in range(xx):\n",
    "            if TF[j][i] >= 1:\n",
    "                r = r + 1\n",
    "        document_frequency.append(r)\n",
    "    \n",
    "    IDF_2 = []\n",
    "    for i in document_frequency:\n",
    "        x = math.log10(xx/i)\n",
    "        IDF_2.append(x)\n",
    "    IDF = pd.DataFrame(IDF_2)\n",
    "    return vectorizer.get_feature_names(), IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = remove_num(w2v_pre_train['Tweet'])\n",
    "s_data = remove_num(w2v_pre_test['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names, IDF_t = get_IDF(n_data, 16041, 29039)\n",
    "test_names, IDF_s = get_IDF(s_data, 3035, 9503)\n",
    "\n",
    "train_names = pd.DataFrame(train_names)\n",
    "test_names = pd.DataFrame(test_names)\n",
    "\n",
    "train_idf = pd.concat([train_names, IDF_t], axis=1)\n",
    "test_idf = pd.concat([test_names, IDF_s], axis=1)\n",
    "\n",
    "train_idf.columns = ['word', 'idf']\n",
    "test_idf.columns = ['word', 'idf']\n",
    "\n",
    "train_t_idf = train_idf.set_index('word')['idf'].to_dict()\n",
    "test_t_idf = test_idf.set_index('word')['idf'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_vec(df, ff):\n",
    "    ww = pd.DataFrame()\n",
    "    for i in df:\n",
    "        w2v2 = pd.DataFrame()\n",
    "        for x in i.split():\n",
    "            if not (x.isdigit() or x[0] == '-' and x[1:].isdigit()):\n",
    "                try:\n",
    "                    w2v = pd.DataFrame(model[x]*(ff['%s'%x]))\n",
    "                    w2v2 = pd.concat([w2v2, w2v], axis=1)\n",
    "                except:\n",
    "                    x\n",
    "        w2v2['SUM'] = w2v2.sum(axis=1)\n",
    "        www = pd.concat([ww, w2v2['SUM']], axis=1)\n",
    "        ww = www\n",
    "    word2v = ww.T.reset_index()\n",
    "    word2vec = word2v.drop(word2v.columns[[0]],axis=1)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embadding_train = word_2_vec(w2v_pre_train['Tweet'], train_t_idf)\n",
    "word_embadding_test = word_2_vec(w2v_pre_test['Tweet'], test_t_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embadding_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_pos_emotions = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "tt_neg_emotions = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "tt_thashtags = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "tt_mentions = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "tt_urls = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "tt_em = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))\n",
    "\n",
    "ss_pos_emotions = pd.DataFrame(np.array(test.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "ss_neg_emotions = pd.DataFrame(np.array(test.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "ss_thashtags = pd.DataFrame(np.array(test.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "ss_mentions = pd.DataFrame(np.array(test.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "ss_urls = pd.DataFrame(np.array(test.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "ss_em = pd.DataFrame(np.array(test.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2 ).fit(All_Data_2['Tweet'])\n",
    "train_data_vectorized = vectorizer.transform(All_Data_2['Tweet'])\n",
    "test_data_vectorized = vectorizer.transform(test['Tweet'])\n",
    "train_labels = All_Data[1]\n",
    "\n",
    "train_d_vec = pd.DataFrame(train_data_vectorized.todense())\n",
    "test_d_vec = pd.DataFrame(test_data_vectorized.todense())\n",
    "\n",
    "train_d = pd.concat([train_d_vec, tt_pos_emotions], axis=1)\n",
    "train_dd = pd.concat([train_d, tt_thashtags], axis=1)\n",
    "train_ddd = pd.concat([train_dd, tt_mentions], axis=1)\n",
    "train_dddd = pd.concat([train_ddd, tt_urls], axis=1)\n",
    "train_ddddd = pd.concat([train_dddd, tt_em], axis=1)\n",
    "train_dddddd = pd.concat([train_ddddd, tt_neg_emotions], axis=1)\n",
    "tt_dd = pd.concat([train_dddddd, word_embadding_train], axis=1)\n",
    "\n",
    "test_d = pd.concat([test_d_vec, ss_pos_emotions], axis=1)\n",
    "test_dd = pd.concat([test_d, ss_thashtags], axis=1)\n",
    "test_ddd = pd.concat([test_dd, ss_mentions], axis=1)\n",
    "test_dddd = pd.concat([test_ddd, ss_urls], axis=1)\n",
    "test_ddddd = pd.concat([test_dddd, ss_em], axis=1)\n",
    "test_dddddd = pd.concat([test_ddddd, ss_neg_emotions], axis=1)\n",
    "ss_dd = pd.concat([test_dddddd, word_embadding_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfr = LogisticRegression()\n",
    "clfr.fit(tt_dd, train_labels)\n",
    "predicted = clfr.predict(ss_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = pd.DataFrame(predicted)\n",
    "result_labels.columns = ['sentiment']\n",
    "result_id = test\n",
    "Results = pd.concat([t_test['id'], result_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results.to_csv(path_or_buf=\"English_Tweets_Results1\", columns=['id', 'sentiment'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
