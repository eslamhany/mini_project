{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from scipy import sparse\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2013train.txt\", sep='\\t', header=None)\n",
    "file_2 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2015train.txt\", sep='\\t', header=None)\n",
    "file_3 = pd.read_csv(\"D:/NU/Data Mining/Mini Project/Training_Data/twitter-2016train.txt\", sep='\\t', header=None)\n",
    "t_test = pd.read_csv(\"D:/NU/Data Mining/Mini Project/new_english_test.csv\", sep=',')\n",
    "Stop_Words = pd.read_csv(\"D:/NU/Data Mining/Mini Project/stopwords.txt\",header=None)\n",
    "all_files = pd.concat([file_1, file_2, file_3], ignore_index=True)\n",
    "All_Data = all_files.drop(all_files.columns[[0]], axis=1)\n",
    "All_Data_2 = All_Data.drop(all_files.columns[[1]], axis=1)\n",
    "Stop_Words = Stop_Words.iloc[3:]\n",
    "Stop_Words.columns = ['Words']\n",
    "StopWords = Stop_Words['Words'].tolist()\n",
    "All_Data_2.columns = ['Tweet']\n",
    "test = t_test.drop(t_test.columns[[0]], axis=1)\n",
    "test.columns = ['Tweet']\n",
    "for a in StopWords:\n",
    "    if isinstance(a, str):\n",
    "        text = a\n",
    "        decoded = False\n",
    "    else:\n",
    "        text = a.decode(encoding)\n",
    "        decoded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(line):\n",
    "    line = line.lower()\n",
    "    curr = \"\"\n",
    "    for word in line.split():\n",
    "        word = re.sub(r'\\\\[u][0-9]{1,}', '', word)\n",
    "        word = re.sub(r'@[a-zA-Z0-9_.-]{1,}','',word)\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", ' ', word)\n",
    "        if word not in StopWords:\n",
    "            curr = curr + word +\" \"\n",
    "    curr = curr.strip()\n",
    "    curr = PorterStemmer().stem(curr)\n",
    "    curr = curr.strip()\n",
    "    return curr\n",
    "\n",
    "def CountEmo_pos(line):\n",
    "    postive = line.count(':)')\n",
    "    postive = postive + line.count(':D')\n",
    "    postive = postive + line.count('=)')\n",
    "    postive = postive + line.count(':*')\n",
    "    postive = postive + line.count(':-)')\n",
    "    postive = postive + line.count('<3')\n",
    "    return postive\n",
    "\n",
    "def CountEmo_neg(line):\n",
    "    negative = line.count(':(')\n",
    "    negative = negative + line.count(':_(')\n",
    "    negative = negative + line.count(':''(')\n",
    "    negative = negative + line.count('=(')\n",
    "    negative = negative + line.count(':-(')\n",
    "    negative = negative + line.count(':S')\n",
    "    return negative\n",
    "\n",
    "def CountHashtags (line):\n",
    "    count = line.count('#')\n",
    "    return count\n",
    "\n",
    "def CountURLS (line):\n",
    "    count = line.count('http')\n",
    "    return count\n",
    "\n",
    "def CountMentions (line):\n",
    "    count = line.count('#')\n",
    "    return count\n",
    "                                         \n",
    "def CountExclamationMark(line):\n",
    "    count = line.count('!')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_2['Positive_Emoticons'] = All_Data_2['Tweet'].apply(CountEmo_pos)\n",
    "All_Data_2['Negative_Emoticons'] = All_Data_2['Tweet'].apply(CountEmo_neg)\n",
    "All_Data_2['HashTags'] = All_Data_2['Tweet'].apply(CountHashtags)\n",
    "All_Data_2['Mentions'] = All_Data_2['Tweet'].apply(CountMentions)\n",
    "All_Data_2['URLS'] = All_Data_2['Tweet'].apply(CountURLS)\n",
    "All_Data_2['EM'] = All_Data_2['Tweet'].apply(CountExclamationMark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_2['Tweet'] = All_Data_2['Tweet'].apply(PreProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_labels,test_labels = train_test_split(All_Data_2,All_Data[1], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pos_emotions = pd.DataFrame(np.array(train_data.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "t_neg_emotions = pd.DataFrame(np.array(train_data.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "t_thashtags = pd.DataFrame(np.array(train_data.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "t_mentions = pd.DataFrame(np.array(train_data.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "t_urls = pd.DataFrame(np.array(train_data.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "t_em = pd.DataFrame(np.array(train_data.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))\n",
    "\n",
    "s_pos_emotions = pd.DataFrame(np.array(test_data.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "s_neg_emotions = pd.DataFrame(np.array(test_data.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "s_thashtags = pd.DataFrame(np.array(test_data.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "s_mentions = pd.DataFrame(np.array(test_data.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "s_urls = pd.DataFrame(np.array(test_data.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "s_em = pd.DataFrame(np.array(test_data.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    vectorizer = CountVectorizer(min_df=2 ).fit(train_data['Tweet'])\n",
    "    train_data_vectorized = vectorizer.transform(train_data['Tweet'])\n",
    "    test_data_vectorized = vectorizer.transform(test_data['Tweet'])\n",
    "\n",
    "    train_d_vec = pd.DataFrame(train_data_vectorized.todense())\n",
    "    test_d_vec = pd.DataFrame(test_data_vectorized.todense())\n",
    "\n",
    "    train_d = pd.concat([train_d_vec, t_pos_emotions], axis=1)\n",
    "    train_dd = pd.concat([train_d, t_thashtags], axis=1)\n",
    "    train_ddd = pd.concat([train_dd, t_mentions], axis=1)\n",
    "    train_dddd = pd.concat([train_ddd, t_urls], axis=1)\n",
    "    train_ddddd = pd.concat([train_dddd, t_em], axis=1)\n",
    "    train_dddddd = pd.concat([train_ddddd, t_neg_emotions], axis=1)\n",
    "\n",
    "    test_d = pd.concat([test_d_vec, s_pos_emotions], axis=1)\n",
    "    test_dd = pd.concat([test_d, s_thashtags], axis=1)\n",
    "    test_ddd = pd.concat([test_dd, s_mentions], axis=1)\n",
    "    test_dddd = pd.concat([test_ddd, s_urls], axis=1)\n",
    "    test_ddddd = pd.concat([test_dddd, s_em], axis=1)\n",
    "    test_dddddd = pd.concat([test_ddddd, s_neg_emotions], axis=1)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "    search_space = [{'classifier': [LogisticRegression()],\n",
    "                     'classifier__penalty': ['l1', 'l2'],\n",
    "                     'classifier__C': np.logspace(0, 4, 10)},\n",
    "                    {'classifier': [RandomForestClassifier()],\n",
    "                     'classifier__n_estimators': [10, 100, 1000],\n",
    "                     'classifier__max_features': [1, 2, 3]}]\n",
    "\n",
    "    clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "    best_model = clf.fit(train_dddddd, train_labels)\n",
    "\n",
    "    print (best_model.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-e5425647df60>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dddddd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess_for_w2v(line):\n",
    "    line = line.lower()\n",
    "    curr = \"\"\n",
    "    for word in line.split():\n",
    "        word = re.sub(r'\\\\[u][0-9]{1,}', '', word)\n",
    "        word = re.sub(r'@[a-zA-Z0-9_.-]{1,}','',word)\n",
    "        word = re.sub(r\"[^a-zA-Z0-9]+\", ' ', word)\n",
    "        if word not in StopWords:\n",
    "            curr = curr + word +\" \"\n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Positive_Emoticons'] = test['Tweet'].apply(CountEmo_pos)\n",
    "test['Negative_Emoticons'] = test['Tweet'].apply(CountEmo_neg)\n",
    "test['HashTags'] = test['Tweet'].apply(CountHashtags)\n",
    "test['Mentions'] = test['Tweet'].apply(CountMentions)\n",
    "test['URLS'] = test['Tweet'].apply(CountURLS)\n",
    "test['EM'] = test['Tweet'].apply(CountExclamationMark)\n",
    "\n",
    "test['Tweet'] = test['Tweet'].apply(PreProcess)\n",
    "\n",
    "w2v_pre_train = pd.DataFrame(All_Data[2].apply(PreProcess_for_w2v))\n",
    "w2v_pre_test = pd.DataFrame(test['Tweet'].apply(PreProcess_for_w2v))\n",
    "\n",
    "w2v_pre_train.columns = ['Tweet']\n",
    "w2v_pre_test.columns = ['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('D:/NU/Data Mining/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All_Data_2['Tweet'] = All_Data_2['Tweet'].apply(PreProcess_for_w2v)\n",
    "test['Tweet'] = test['Tweet'].apply(PreProcess_for_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(df):\n",
    "    dd_ff = []\n",
    "    for i in df:\n",
    "        ww = \"\"\n",
    "        for x in i.split():\n",
    "            if not (x.isdigit() or x[0] == '-' and x[1:].isdigit()):\n",
    "                ww = ww + x +\" \"\n",
    "        dd_ff.append(ww)\n",
    "    n_data = pd.DataFrame(dd_ff)\n",
    "    return dd_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDF(corpus, xx, yy):\n",
    "    vectorizer = CountVectorizer()\n",
    "    cvect = vectorizer.fit_transform(corpus)\n",
    "    analyze = vectorizer.build_analyzer()\n",
    "    TF = cvect.toarray()\n",
    "    \n",
    "    document_frequency = []\n",
    "    for i in range(yy):\n",
    "        r = 0\n",
    "        for j in range(xx):\n",
    "            if TF[j][i] >= 1:\n",
    "                r = r + 1\n",
    "        document_frequency.append(r)\n",
    "    \n",
    "    IDF_2 = []\n",
    "    for i in document_frequency:\n",
    "        x = math.log10(xx/i)\n",
    "        IDF_2.append(x)\n",
    "    IDF = pd.DataFrame(IDF_2)\n",
    "    return vectorizer.get_feature_names(), IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = remove_num(w2v_pre_train['Tweet'])\n",
    "s_data = remove_num(w2v_pre_test['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names, IDF_t = get_IDF(n_data, 16041, 29039)\n",
    "test_names, IDF_s = get_IDF(s_data, 3035, 9503)\n",
    "\n",
    "train_names = pd.DataFrame(train_names)\n",
    "test_names = pd.DataFrame(test_names)\n",
    "\n",
    "train_idf = pd.concat([train_names, IDF_t], axis=1)\n",
    "test_idf = pd.concat([test_names, IDF_s], axis=1)\n",
    "\n",
    "train_idf.columns = ['word', 'idf']\n",
    "test_idf.columns = ['word', 'idf']\n",
    "\n",
    "train_t_idf = train_idf.set_index('word')['idf'].to_dict()\n",
    "test_t_idf = test_idf.set_index('word')['idf'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_vec(df, ff):\n",
    "    ww = pd.DataFrame()\n",
    "    for i in df:\n",
    "        w2v2 = pd.DataFrame()\n",
    "        for x in i.split():\n",
    "            if not (x.isdigit() or x[0] == '-' and x[1:].isdigit()):\n",
    "                try:\n",
    "                    w2v = pd.DataFrame(model[x]*(ff['%s'%x]))\n",
    "                    w2v2 = pd.concat([w2v2, w2v], axis=1)\n",
    "                except:\n",
    "                    x\n",
    "        w2v2['SUM'] = w2v2.sum(axis=1)\n",
    "        www = pd.concat([ww, w2v2['SUM']], axis=1)\n",
    "        ww = www\n",
    "    word2v = ww.T.reset_index()\n",
    "    word2vec = word2v.drop(word2v.columns[[0]],axis=1)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embadding_train = word_2_vec(w2v_pre_train['Tweet'], train_t_idf)\n",
    "word_embadding_test = word_2_vec(w2v_pre_test['Tweet'], test_t_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embadding_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_pos_emotions = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "tt_neg_emotions = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "tt_thashtags = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "tt_mentions = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "tt_urls = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "tt_em = pd.DataFrame(np.array(All_Data_2.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))\n",
    "\n",
    "ss_pos_emotions = pd.DataFrame(np.array(test.drop(['Tweet', 'Negative_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "ss_neg_emotions = pd.DataFrame(np.array(test.drop(['Tweet', 'Positive_Emoticons','HashTags', 'Mentions', 'URLS', 'EM'],1)))\n",
    "ss_thashtags = pd.DataFrame(np.array(test.drop(['Tweet', 'Negative_Emoticons','Positive_Emoticons', 'Mentions', 'URLS', 'EM'],1)))\n",
    "ss_mentions = pd.DataFrame(np.array(test.drop(['Tweet', 'HashTags', 'Negative_Emoticons','Positive_Emoticons', 'URLS', 'EM'],1)))\n",
    "ss_urls = pd.DataFrame(np.array(test.drop(['Tweet', 'HashTags', 'Mentions', 'Negative_Emoticons','Positive_Emoticons', 'EM'],1)))\n",
    "ss_em = pd.DataFrame(np.array(test.drop(['Tweet', 'HashTags', 'Mentions', 'URLS', 'Negative_Emoticons','Positive_Emoticons'],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2 ).fit(All_Data_2['Tweet'])\n",
    "train_data_vectorized = vectorizer.transform(All_Data_2['Tweet'])\n",
    "test_data_vectorized = vectorizer.transform(test['Tweet'])\n",
    "train_labels = All_Data[1]\n",
    "\n",
    "train_d_vec = pd.DataFrame(train_data_vectorized.todense())\n",
    "test_d_vec = pd.DataFrame(test_data_vectorized.todense())\n",
    "\n",
    "train_d = pd.concat([train_d_vec, tt_pos_emotions], axis=1)\n",
    "train_dd = pd.concat([train_d, tt_thashtags], axis=1)\n",
    "train_ddd = pd.concat([train_dd, tt_mentions], axis=1)\n",
    "train_dddd = pd.concat([train_ddd, tt_urls], axis=1)\n",
    "train_ddddd = pd.concat([train_dddd, tt_em], axis=1)\n",
    "train_dddddd = pd.concat([train_ddddd, tt_neg_emotions], axis=1)\n",
    "tt_dd = pd.concat([train_dddddd, word_embadding_train], axis=1)\n",
    "\n",
    "test_d = pd.concat([test_d_vec, ss_pos_emotions], axis=1)\n",
    "test_dd = pd.concat([test_d, ss_thashtags], axis=1)\n",
    "test_ddd = pd.concat([test_dd, ss_mentions], axis=1)\n",
    "test_dddd = pd.concat([test_ddd, ss_urls], axis=1)\n",
    "test_ddddd = pd.concat([test_dddd, ss_em], axis=1)\n",
    "test_dddddd = pd.concat([test_ddddd, ss_neg_emotions], axis=1)\n",
    "ss_dd = pd.concat([test_dddddd, word_embadding_test], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tttt = np.where(tt_dd.values >= np.finfo(np.float64).max)\n",
    "ssss = np.where(ss_dd.values >= np.finfo(np.float64).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfr = LogisticRegression()\n",
    "clfr.fit(tt_dd, train_labels)\n",
    "predicted = clfr.predict(ss_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_labels = pd.DataFrame(predicted)\n",
    "result_labels.columns = ['sentiment']\n",
    "result_id = test\n",
    "Results = pd.concat([t_test['id'], result_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results.to_csv(path_or_buf=\"English_Tweets_Results1\", columns=['id', 'sentiment'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
